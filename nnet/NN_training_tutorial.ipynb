{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1: Load yass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import progressbar\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import scipy.io as sio\n",
    "import yass\n",
    "from yass import read_config\n",
    "from yass import preprocess\n",
    "from yass.augment import make_training_data, save_detect_network_params, save_triage_network_params, save_ae_network_params\n",
    "from yass.augment import train_detector, train_ae, train_triage\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2: Read Configuration File**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yass.set_config(\"location/to/config.yaml\")\n",
    "CONFIG = read_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2.1: Preprocessing (optional)**\n",
    "\n",
    "Make sure the standarized data exists. Otherwise, run this step to automatically generate a temp folder including the standarized and filtered data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3: Load Spike Train**\n",
    "\n",
    "To train the Neural Network, you need to have a recording with sorted result. The result does not need to be perfect.\n",
    "If you don't have any sorting result yet, you can run yass with threshold detection option. In your configuration file, set spikes.detection = threshold.\n",
    "\n",
    "spike_train is a matrix of size (number of spikes x 2). Each row represents an individual spike. The first column is the spike time (not in milliseconds or seconds but in actual temporal location in recording). The second column is the spike ID.\n",
    "\n",
    "Other python functions can be used to load the data depending on the data type. For example, use the following code for loading the binary data: \n",
    "\n",
    "f = open('location/to/spiketrain', 'r')\n",
    "\n",
    "spike_train= np.fromfile(f, dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spike_train=np.loadtxt(\"location/to/spiketrain\",dtype=int) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3.2: Visually Inspect Templates (optional)**\n",
    "\n",
    "Not necessarily all the templates generated from the groundtruth look good. It is advised that user visually inspect these templates one by one and only keep good templates, i.e. isolated spikes with enough absolute amplitude (usually >= 3 standard units)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from yass.templates.util import get_templates\n",
    "from yass.augment.crop import crop_templates\n",
    "\n",
    "#if the standardized data is elsewhere, replace the following path accordingly\n",
    "path_to_data = os.path.join(CONFIG.data.root_folder, 'tmp/standarized.bin') \n",
    "\n",
    "templates, _ = get_templates(spike_train, path_to_data, 4*CONFIG.spike_size)\n",
    "templates = np.transpose(templates, (2, 1, 0))\n",
    "templates = crop_templates(templates, CONFIG.spike_size,\n",
    "                           CONFIG.neigh_channels, CONFIG.geom)\n",
    "for i in range(templates.shape[0]):\n",
    "    plt.plot(templates[i])\n",
    "    plt.title('template id: %d'%(i))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4: Make Training Dataset**\n",
    "\n",
    "1. CONFIG and spike_train are from step 2 and 3.\n",
    "\n",
    "2. chosen_templates: It is a vector containing which templates to use. Given spike sorting result, not all templates look good. Therefore, the training dataset should be obtained from good looking templates only. Make sure that you do not include bad templates. However, it is still important to keep variability in template shapes. To visually check templates, check optional step at the bottom.\n",
    "\n",
    "3. min_amp: the minimum of absolute maximal amplitude of augmented spikes. It should determine how small spikes in the training set can be. Default is 3.\n",
    "\n",
    "4. nspikes: approximately how many training data it should produce? This is the number of isolated spikes in the training set, NOT the total number of training datapoints \n",
    "\n",
    "5. noise_ratio,collision_ratio,misalign_ratio,misalign_ratio2: the ratio of the number of corresponding type of training data as opposed to the number of isolated spikes. For example, if noise_ratio=10, collision_ratio=1, misalign_ratio=3, misalign_ratio2=3; then for every portion of isolated data generated, 10 portions of noise data, 1 portion of collision data, 3 portions of spatially-and-temporally misaligned data (for x_detect), and 3 portions of spatially misaligned data(x_triage) will be generated. The proportion is to be customized according to the assumption of the distribution of the portions in real data. It is generally recommended that noise_ratio>=5 to reflect the sparse firing pattern of the neurons.\n",
    "\n",
    "6. multi: boolean variable, if True (default), multi-channel data will be generated.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_amp = 3\n",
    "nspikes = 10000\n",
    "data_folder=\"location/to/standarized.bin\"\n",
    "chosen_templates=np.arange(1,164) # should be your own numbers\n",
    "np.delete(chosen_templates,[12,34,57,65,163])#getting rid of the bad-looking templates from Step 3.2\n",
    "noise_ratio=10\n",
    "collision_ratio=1\n",
    "misalign_ratio=3\n",
    "misalign_ratio2=3\n",
    "multi=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_detect, y_detect, x_triage, y_triage, x_ae, y_ae = make_training_data(CONFIG, spike_train, chosen_templates,\n",
    "                                                                         min_amp, nspikes,data_folder,\n",
    "                                                                         noise_ratio,collision_ratio,misalign_ratio,\n",
    "                                                                         misalign_ratio2,multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 5: Train All Three Neural Networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training parameters:\n",
    "1. n_iter: the number of iterations to run\n",
    "2. n_batch: the size of mini-batch to be used for training\n",
    "3. l2_reg_scale: L2 regularization penalty term\n",
    "4. train_step_size: training step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50000\n",
    "n_batch = 512\n",
    "l2_reg_scale = 0.00000005\n",
    "train_step_size =  0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training neural net detector\n",
    "1. detect_name: name of saved model with the location to save \n",
    "2. n_filters: number of filters to use in each layer. It should be a list of length 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectnet_name = 'detectnet'\n",
    "n_filters_detect = [16, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_detector(x_detect, y_detect, n_filters_detect, n_iter, n_batch, l2_reg_scale, train_step_size, detectnet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "save_detect_network_params(filters = n_filters_detect,\n",
    "                           size = x_detect.shape[1],\n",
    "                           n_neighbors = x_detect.shape[2],\n",
    "                           output_path = detectnet_name.replace('ckpt', 'yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training neural net triage\n",
    "1. triage_name: name of saved model with the location to save \n",
    "2. n_filters: number of filters to use in each layer. It should be a list of length 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triagenet_name = 'triagenet'\n",
    "n_filters_triage = [16, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training\n",
    "train_triage(x_triage, y_triage, n_filters_triage, n_iter, n_batch, l2_reg_scale, train_step_size, triagenet_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "save_triage_network_params(filters = n_filters_triage,\n",
    "                           size = x_detect.shape[1],\n",
    "                           n_neighbors = x_detect.shape[2],\n",
    "                           output_path = triagenet_name.replace('ckpt', 'yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training autoencoder\n",
    "1. ae_name: name of saved model with the location to save \n",
    "2. n_feature: number of latent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ae_name = 'ae'\n",
    "n_features = 3\n",
    "n_batch = x_ae.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# run training\n",
    "train_ae(x_ae, y_ae, n_features, n_iter, n_batch, train_step_size, ae_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model parameters\n",
    "save_ae_network_params(n_input = x_ae.shape[1],\n",
    "                       n_features = n_features,\n",
    "                       output_path = ae_name.replace('ckpt', 'yaml'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You are done!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 6: When Using yass**\n",
    "\n",
    "Make sure that you have all your files! You must have **3 '.ckpt'** files and **1 '.yaml'** file for **each neural network model**, which make **total 12 files**.\n",
    "\n",
    "Also, make sure that the parameters in your configuration file match with the parameters used during the training\n",
    "\n",
    "| Name in config.yaml | How it should change |\n",
    "|---|---|\n",
    "|spikes.temporal_features|n_feature used for training autoencoder|\n",
    "|recordings.spike_size_ms|make sure that this value stays the same as configuration loaded here|\n",
    "|neural_network_detector.filename|file name used above to save neural net detector|\n",
    "|neural_network_triage.filename|file name used above to save neural net triage|\n",
    "|neural_network_autoencoder.filename|file name used above to save neural net autoencoder|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
