{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generating simulated testing data ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peter/miniconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "from yass.augment.choose import choose_templates\n",
    "from yass.augment.crop import crop_templates\n",
    "from yass.augment.noise import noise_cov\n",
    "from yass.templates.util import get_templates\n",
    "from yass.util import load_yaml\n",
    "\n",
    "# TODO: documentation\n",
    "# TODO: comment code, it's not clear what it does\n",
    "def make_testing_data(CONFIG, data_length, ptp_att_std, spike_train, chosen_templates,n_per_cluster,data_folder):\n",
    " \n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    path_to_data = os.path.join(data_folder, 'standarized.bin')\n",
    "    path_to_config = os.path.join(data_folder, 'standarized.yaml')\n",
    "\n",
    "    # make sure standarized data already exists\n",
    "    if not os.path.exists(path_to_data):\n",
    "        raise ValueError('Standarized data does not exist in: {}, this is '\n",
    "                         'needed to generate training data, run the '\n",
    "                         'preprocesor first to generate it'\n",
    "                         .format(path_to_data))\n",
    "\n",
    "    PARAMS = load_yaml(path_to_config)\n",
    "\n",
    "    logger.info('Getting templates...')\n",
    "\n",
    "    # get templates\n",
    "    templates, _ = get_templates(spike_train, path_to_data, data_length*CONFIG.spike_size)\n",
    "\n",
    "    templates = np.transpose(templates, (2, 1, 0))\n",
    "\n",
    "    logger.info('Got templates ndarray of shape: {}'.format(templates.shape))\n",
    "\n",
    "    # choose good templates (good looking and big enough)\n",
    "    #templates = choose_templates(templates, chosen_templates)\n",
    "    templates=templates[chosen_templates]\n",
    "    if templates.shape[0] == 0:\n",
    "        raise ValueError(\"Coulndt find any good templates...\")   \n",
    "\n",
    "    logger.info('Good looking templates of shape: {}'.format(templates.shape))\n",
    "\n",
    "    # align and crop templates\n",
    "    templates = crop_templates(templates, data_length*CONFIG.spike_size,\n",
    "                               CONFIG.neigh_channels, CONFIG.geom)\n",
    "    \n",
    "    # determine noise covariance structure\n",
    "    spatial_SIG, temporal_SIG = noise_cov(path_to_data,\n",
    "                                          PARAMS['dtype'],\n",
    "                                          CONFIG.recordings.n_channels,\n",
    "                                          PARAMS['data_order'],\n",
    "                                          CONFIG.neigh_channels,\n",
    "                                          CONFIG.geom,\n",
    "                                          templates.shape[1])\n",
    "    \n",
    "    # make training data set\n",
    "    K = templates.shape[0]\n",
    "    #R = CONFIG.spike_size\n",
    "    #amps = np.max(np.abs(templates), axis=1)\n",
    "\n",
    "    # make clean augmented spikes\n",
    "    #nk = int(np.ceil(nspikes/K))\n",
    "    #if max_amp == 0:\n",
    "    #    max_amp = np.max(amps)*1.5\n",
    "     \n",
    "    #nneigh = templates.shape[2]\n",
    "\n",
    "    ################\n",
    "    # clean spikes #\n",
    "    ################\n",
    "    x_clean = np.zeros((n_per_cluster*K, templates.shape[1], templates.shape[2]))\n",
    "    ids=np.zeros(x_clean.shape[0],dtype=int)\n",
    "    ptp=np.zeros(K)\n",
    "\n",
    "    for i in range(K):\n",
    "        ptp[i]=np.ptp(templates[i,:,0])\n",
    "        \n",
    "    for k in range(K):\n",
    "        \n",
    "        \n",
    "        tt = templates[k]\n",
    "        ptp_now=ptp[k]\n",
    "        ptp_range = (np.random.normal(1,ptp_att_std,n_per_cluster))[:, np.newaxis, np.newaxis]\n",
    "        \n",
    "        \n",
    "        \n",
    "        x_clean[k*n_per_cluster:(k+1)*n_per_cluster] = tt[np.newaxis, :, :]*ptp_range\n",
    "        ids[k*n_per_cluster:(k+1)*n_per_cluster]=k\n",
    "\n",
    "            \n",
    "    \n",
    "    #########\n",
    "    # noise #\n",
    "    #########\n",
    "\n",
    "    # get noise\n",
    "    noise = np.random.normal(size=[x_clean.shape[0], templates.shape[1], templates.shape[2]])\n",
    "    for c in range(noise.shape[2]):\n",
    "        noise[:, :, c] = np.matmul(noise[:, :, c], temporal_SIG)\n",
    "\n",
    "        reshaped_noise = np.reshape(noise, (-1, noise.shape[2]))\n",
    "    noise = np.reshape(np.matmul(reshaped_noise, spatial_SIG),\n",
    "                       [noise.shape[0], x_clean.shape[1], x_clean.shape[2]])\n",
    "       \n",
    "\n",
    "\n",
    "    x_clean=x_clean+noise\n",
    "    \n",
    "   \n",
    "\n",
    "    return x_clean,noise,ids,ptp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Loading files (please have your config file ready) ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#import h5py\n",
    "import progressbar\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "#import panda as pd\n",
    "import pickle\n",
    "import logging\n",
    "import scipy.io as sio\n",
    "import yass\n",
    "from yass import read_config\n",
    "from yass.augment import make_training_data, save_detect_network_params, save_triage_network_params, save_ae_network_params\n",
    "from yass.augment import train_detector, train_ae, train_triage\n",
    "from yass import preprocess\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "yass.set_config(\"/ssd/data/shenghao/retinal/configuration_retinal.yaml\")\n",
    "CONFIG = read_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Load Spike Train***\n",
    "\n",
    "To train the Neural Network, you need to have a recording with sorted result. The result does not need to be perfect.\n",
    "If you don't have any sorting result yet, you can run yass with threshold detection option. In your configuration file, set spikes.detection = threshold.\n",
    "\n",
    "spike_train is a matrix of size (number of spikes x 2). Each row represents an individual spike. The first column is the spike time (not in milliseconds or seconds but in actual temporal location in recording). The second column is the spike ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 17  39]\n",
      " [ 23  26]\n",
      " [ 61  28]\n",
      " [ 80  15]\n",
      " [126  10]\n",
      " [251  40]\n",
      " [278   6]\n",
      " [288  42]\n",
      " [468  36]\n",
      " [545  38]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# load ground truth\n",
    "# make spikeTrain\n",
    "import scipy.io\n",
    "kk = scipy.io.loadmat('/ssd/data/shenghao/retinal/groundtruth_ej49_data1_set1.mat')\n",
    "\n",
    "#L_gt has length total number of spikes; it is the cluster index for each spike\n",
    "#spt_gt has length total number of spikes; it is the time for each spike\n",
    "L_gt = kk['L_gt']-1\n",
    "spt_gt = kk['spt_gt'] +10\n",
    "spike_train = np.concatenate((spt_gt, L_gt),axis=1)\n",
    "print (spike_train[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***generating testing data!!***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input parameters for x_test\n",
    "\n",
    "data_length=12\n",
    "n_per_cluster=200\n",
    "ptp_att_std=0.01\n",
    "data_folder='/ssd/data/shenghao/retinal/tmp'\n",
    "chosen_templates = [0, 1, 2, 3, 4, 5, 7, 8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 44, 45, 46, 47, 48] # should be your own number\n",
    "x_clean,noise,ids,ptp=make_testing_data(CONFIG, data_length, ptp_att_std, spike_train, chosen_templates,n_per_cluster,data_folder)\n",
    "sio.savemat('retinal_testing_may19.mat',mdict={'x_clean':x_clean,'noise':noise,'ids':ids,'ptp':ptp})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
