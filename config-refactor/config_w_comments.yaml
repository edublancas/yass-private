# idea 1: make some parameters optional if they are not going to be
# modified by most users, provided sane defaults

# DATA
# data root folder
root: data/
# binary recording filename
filename: neuropixel.bin
# channel geometry file name
geomFile: neuropixel_channels.npy
# save spikeTrain
spikeTrainName: spike_train.csv

# RECORDING
# precision of the recording (e.g.'int16')
dtype: int16
# recording rate (in Hz)
srate: 30000
# number of channels in the recording
nChan: 10
# spatial radius of spikes
spatialRadius: 70

# TURN ON/OFF MODULES
# evaluate how often the user would want to turn this off...
doFilter: True
doTriage: True
doCoreset: True
doWhitening: True
# 'nn' for neural net detction, 'threshold' for amplitdue threshold detection
detctionMethod: threshold
# 'MFM' for mixture of finite mixture clustering, 'dpmm' for dirichlete process mixture model
clusteringMethod: MFM
doDeconv: True # i think this should always be true, in which cases the user woulnd't want to do it?

# PREPROCESS
portion: 1 # document this
# max. memory allowed for loading recording (in bytes)
maxMem: 1000000000
# [0,1] to use parital data to estimate templates
partialDat: 1  # document this
# determine whitening matrix in each minibatch
whitenBatchwise: 0
# number of features for dimension reduction
nFeat: 3
# temporal length of wavforms in ms
spikeSizeMS: 1

# NN DETECTION
# improve documenttion for this, optional?
# Threshold for spike event detection
nnThreshdold: 0.5
# Threshold for clear/collision detection
nnThreshdoldCol: 0.75

# FILTERING
# maybe optional...
# Order of Butterworth filter
filterOrder: 3
# Low pass frequency (Hz)
filterLow: 300
# High pass factor (proportion of sampling rate)
filterHighFactor: 0.1

# TRIAGE
# maybe optional...
# Num. of nearest neighbors to consider
triageK: 20
# percentage of data to be triaged
triagePercent: 0.01

# CORESET
# maybe optional...
# Num. of clusters
coresetK: 10 # document this
# distance threshold
coresetTh: 0.95 # document this

# CLUSTERING
# maybe optional...
# Masking threshold
maskTh: [0.9, 0.5] # document this
# Num. of new clusters in split
kSplit: 5 # document this

# TEMPLATE MERGING
# maybe optional...
# threshold for template merge
tMergeTh: [0.8, 0.7] # document this

# DECONVOLUTION
deconvGPU: 1 # we should probably have one general cpu/gpu option
deconvRank: 3 # document this
deconvTh: 4 # document this
deconvLam: 20 # document this

# Neural network parameters
neural_network:
  # NN filenames
  # we need to document the models that come with yass, and here users
  # can select them, also, they should be able to specify custom ckpts
  # tf nn trained model file name
  #nnFilename: 2class_multi2.ckpt
  nnFilename: detectnet1.ckpt
  aeFilename: ae_31.ckpt
  nnTriageFilename: triagenet1.ckpt

  # NN architecture
  # this should be removed as it is model-dependent, we need to find a way
  # to provide this information along with the ckpt file and bundle this
  # in the package. for users providing their own neural nets, we need
  # to find a way for them to provide this settings
  # num. of filters in each layer
  nnNFilters: [16, 8, 1]
  # temporal filter size
  nnFilterSize: [31, 1, 1]  
  
  nnTriageFilterSize: [8, 16]  
  
  # NN training
  # this whole section should be moved from here, as it has to do with nn
  # training and now with the use of the pipeline per se. since there are
  # just a few parameters maybe we can just include this as parameters
  # in the functions used for training neural nets
  # train nn and save
  nnTrain: 0
  # 'recording' : raw recording,
  inputType: 'recording'
  # 'sorted_recording' : raw recording with spike train, 'templates' : templates
  # 'rectangular', 'non_rectangular'
  ChannelGeometry: 'rectangular'
  # num. of iter. in training
  nnIteration: 100000
  # size of minibatch in training
  nnBatch: 256
  # training step size
  nnTrainStepSize: 0.001
  # L2 reg. scale
  nnL2RegScale: 0.00000005

# Clustering prior
# i think we should make this optional
cluster_prior:
  beta: 1
  a: 1
  lambda0: 0.01
  mu: [[0], [0], [0]]
  nu: 5
  V: 2
